{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 - Internal Representation of Numbers & its Effects on Algorithms\n",
    "-------------------\n",
    "# Contents\n",
    "- Introduction to numerical representation\n",
    "    - Integers - signed, unsigned\n",
    "    - Floating-points - 32-bit, 64-bit\n",
    "- Floating point (FP) arithmetic - issues and limitations\n",
    "    - Representation error\n",
    "    - Aliasing error\n",
    "    - Type casting error\n",
    "    - Overflow error\n",
    "    - Roundoff error\n",
    "- Effects of FP datatypes on 2D convolution results\n",
    "    - Run-time\n",
    "    - Memory usage over time\n",
    "    - Precision of result\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will look at how different datatypes can have an impact on the computation efficiency in terms of both time and memory. Furthermore, we will see what's the cost we pay for speed/memory improvements by actively managing datatypes with numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, data is stored in memory in the form of binary bits. Interestingly, the same set of bits can mean different values when interpreted as different data types. Being a high-level programming language, Python keeps these tedious calculations behind the curtain most of the time. However, datatype is still an important aspect when it comes to numerical calculations.\n",
    "\n",
    "### Integers\n",
    "\n",
    "**10110101**\n",
    "\n",
    "Unsigned integer: 181\n",
    "\n",
    "Signed integer: -75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bit_string = \"10110101\"\n",
    "x = int(bit_string, 2)\n",
    "print(\"decimal: \", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(2**7) + (2**5) + (2**4) + (2**2) + (2**0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "unsigned_int = np.uint8(x) # unsigned\n",
    "print(\"unsigned: \", unsigned_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "signed_int = np.int8(x) # signed\n",
    "print(\"signed: \", signed_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Why the Error???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-(2**7) + (2**5) + (2**4) + (2**2) + (2**0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more - https://cheever.domains.swarthmore.edu/Ref/BinaryMath/NumSys.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Floating points\n",
    "\n",
    "![](fp_representation.jpg)\n",
    "\n",
    "IEEE-754 converter: [https://www.h-schmidt.net/FloatConverter/IEEE754.html](https://www.h-schmidt.net/FloatConverter/IEEE754.html)\n",
    "\n",
    "Watch [this video](https://www.youtube.com/watch?v=RuKkePyo9zk) (21:34) to see how \"decimal points\" work in computer memory.\n",
    "\n",
    "Numpy's floating point datatypes comforms to IEEE-754 floating point standard, and match the underlying C language data types:\n",
    "\n",
    "|Numpy Type | C Type|\n",
    "|---|----|\n",
    "|np.float32 |\tfloat|\n",
    "|np.float64 |\tdouble|\n",
    "|np.float96 |\tlong double|\n",
    "|np.float128 | long double|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Exercise] What's an advantage of using a 64-bit float as compared to 32-bit? What's an disadvantage?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read about [extended precision floating point types in Numpy](https://numpy.org/devdocs/user/basics.types.html#extended-precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer:*\n",
    "\n",
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representation error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representation error refers to the fact that some (most, actually) decimal fractions cannot be represented exactly as binary (base 2) fractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".1 + .1 + .1 == .3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is that? 1/10 is not exactly representable as a binary fraction. Almost all platforms map Python floats to IEEE-754 “double precision”. **754 doubles contain 53 bits of precision, so the computer strives to convert 0.1 to the closest fraction it can of the form J / 2\\*\\*N where J is an integer containing exactly 53 bits.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e. 1/10 ~= J / 2\\*\\*N.\n",
    "\n",
    "Let's try to find J and N on our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 56\n",
    "\n",
    "2**52 <=  2**N // 10  < 2**53"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, 56 is the only value for N that leaves J with exactly 53 bits. The best possible value for J is then that quotient rounded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, r = divmod(2**56, 10)\n",
    "print(q)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since remainder is closer to 10, round up q by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = q + 1\n",
    "J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_power_N = 2 ** 56\n",
    "two_power_N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore best possible approximation of 1/10 is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format(J / two_power_N, '.55f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exactly the decimal value that is being represented internally. Let's confirm our calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "Decimal.from_float(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_str = \"{0:b}\".format(7205759403792794) # J\n",
    "print (bin_str)\n",
    "len(bin_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get `J` and `two_power_N` more easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fractions import Fraction\n",
    "\n",
    "print(Fraction.from_float(0.1))\n",
    "print((0.1).as_integer_ratio())\n",
    "\n",
    "# 7205759403792794 / 2 ** 56 --> divide by 2 --> 3602879701896397 / 2 ** 55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the same `J` (numerator) and `two_power_N` (denominator) values we found before, but both divided by 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Exercise] What integer fraction does 3.14159 actually equal to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 3.14159"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert x == ??? / ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aliasing errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "val = np.float32(0.1)\n",
    "print(val.dtype)\n",
    "format(val, '.55f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format(np.float64(val), '.55f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect the representational error of a double to be smaller than that of a float. However, casting the float to a double has the effect of persisting the representational error of the float since enough 0’s are added to the right of the significand value to pad out the significand of a double."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tiny numbers\n",
    "import struct\n",
    "\n",
    "def float_to_binary(number):\n",
    "    return format(struct.unpack('!I', struct.pack('!f', number))[0], '032b')\n",
    "\n",
    "print(float_to_binary(0.1))\n",
    "\n",
    "tiny_number = 1.4013e-45  # This is close to the smallest representable positive number in single-precision.\n",
    "print(float_to_binary(tiny_number))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type casting errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([3.2, -1.0, 6.8, 10.12])\n",
    "print(arr, arr.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overflow error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.power(100, 8, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.power(100, 8, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_till_overflow(number):\n",
    "    i = 1\n",
    "    while number != float('inf'):\n",
    "        number *= 2\n",
    "        print(f\"Iteration {i}: {number}\")\n",
    "        i += 1\n",
    "\n",
    "multiply_till_overflow(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Task** How many iterations were needed??\n",
    "\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the information for the data types\n",
    "print(np.iinfo(np.int32))\n",
    "print(np.iinfo(np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Similar for floating point\n",
    "print(np.finfo(np.float16))\n",
    "print(np.finfo(np.float32))\n",
    "print(np.finfo(np.float64))\n",
    "print(np.finfo(np.float128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roundoff Error\n",
    "\n",
    "When two floating point numbers are operated on, their representation is adjusted such that their exponents match the higher value of the two. This also requires the significand to be adjusted such that the original value is still represented. \n",
    "\n",
    "When two floating point numbers are operated on, their representation is adjusted such that their exponents match the higher value of the two. This also requires the significand to be adjusted such that the original value is still represented. When this occurs, since the exponent of the floating point is usually increase, the significand gets right shifted. **The problem arises is when the significand starts to loose bits of the right side due to the limited number of bits in the significand. This causes the significant digits of the significand to be reduced and the value represented rounded off as a result, hence the reason it is called round off error.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loss of Significance\n",
    "a = 1.123456789\n",
    "b = 1.123456788\n",
    "difference = a - b\n",
    "print(f\"Difference: {difference}\")\n",
    "# This value might be smaller than expected due to the loss of significance in floating-point arithmetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "values = np.full(shape=100, fill_value=10, dtype=np.float32) # Return a new array of given shape and type, filled with fill_value.\n",
    "sum = np.float32(100000000)\n",
    "for val in values:\n",
    "    sum += val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're adding ten 100 times, we expect to see 100001000.0 but `sum` falls short of this result. Let's try to capture this error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum(dtype):\n",
    "    values = np.full(shape=100, fill_value=10, dtype=dtype) # Return a new array of given shape and type, filled with fill_value.\n",
    "    sum = dtype(100000000)\n",
    "\n",
    "    for val in values:\n",
    "        temp_sum = sum + val # sum two components\n",
    "        error = (temp_sum - sum) - val # from the sum, subtract both components, should get ideally 0 but there will be a non-zero error\n",
    "        sum = temp_sum # update for next iteration\n",
    "\n",
    "    return sum, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f32_sum, f32_error = get_sum(np.float32)\n",
    "f64_sum, f64_error = get_sum(np.float64)\n",
    "print(f\"f32: {format(f32_sum, '.1f')} {format(f32_error, '.1f')}\")\n",
    "print(f\"f64: {format(f64_sum, '.1f')} {format(f64_error, '.1f')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about how mitigate this error - https://diybigdata.net/2016/07/data-science-and-floating-point-arithmetic/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effects on some simple functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Iterative Summation\n",
    "numbers = [0.1]*10000000\n",
    "expected_sum = 0.1 * 10000000\n",
    "\n",
    "# Iterative summation\n",
    "actual_sum = np.sum(numbers)\n",
    "\n",
    "print(f\"Expected Sum: {expected_sum}\")\n",
    "print(f\"Actual Sum: {actual_sum}\")\n",
    "print(f\"Error: {expected_sum - actual_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Polynomial Roots\n",
    "coefficients = [1, -6, 11.1, -6.1]  # Roots are 1, 2, and 3.1, but the coefficients are slightly perturbed.\n",
    "roots = np.roots(coefficients)\n",
    "\n",
    "print(f\"Roots: {roots}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix inversion\n",
    "# try different values for n\n",
    "\n",
    "def hilbert_matrix(n):\n",
    "    \"\"\"Generate an n x n Hilbert matrix.\"\"\"\n",
    "    H = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            H[i, j] = 1.0 / (i + j + 1)\n",
    "    return H\n",
    "\n",
    "matrix = hilbert_matrix(4)\n",
    "print(\"Hilbert Matrix (4x4):\")\n",
    "print(matrix)\n",
    "\n",
    "inverse = np.linalg.inv(matrix)\n",
    "print(\"\\nInverse of the Hilbert Matrix:\")\n",
    "print(inverse)\n",
    "\n",
    "product = np.dot(matrix, inverse)\n",
    "print(\"\\nHilbert Matrix * Its Inverse (which should be the identy matrix):\")\n",
    "print(product)  ## Should be identity matrix\n",
    "print(\"\\nIdentity Matrix:\")\n",
    "print(np.eye(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = np.linalg.norm(product - np.eye(4))\n",
    "print(f\"\\nError from Identity: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More on Hilbert Matrix: https://en.wikipedia.org/wiki/Hilbert_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Task**: Write a for loop code that keeps track for the error for different values of n (from 2 to 12) and plot the error as computed above vs n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: what do you notice as you try different values of n?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "\n",
    "### Effects of datatypes on 2D convolution results - Run time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the memory profiler library to analyze the memory utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install memory_profiler\n",
    "%load_ext memory_profiler\n",
    "\n",
    "from skimage.io import imread, imshow, imsave\n",
    "from skimage.filters import gaussian\n",
    "from scipy.signal import convolve2d\n",
    "from skimage.color import rgb2gray\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "%matplotlib inline\n",
    "from memory_profiler import memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve2D(image, kernel, padding=0):\n",
    "    # Cross Correlation\n",
    "    kernel = np.flipud(np.fliplr(kernel))\n",
    "\n",
    "    # Gather Shapes of Kernel + Image + Padding\n",
    "    xKernShape = kernel.shape[0]\n",
    "    yKernShape = kernel.shape[1]\n",
    "    xImgShape = image.shape[0]\n",
    "    yImgShape = image.shape[1]\n",
    "\n",
    "    # Shape of Output Convolution\n",
    "    xOutput = int((xImgShape - xKernShape + 2 * padding) + 1)\n",
    "    yOutput = int((yImgShape - yKernShape + 2 * padding) + 1)\n",
    "    output = np.zeros((xOutput, yOutput))\n",
    "\n",
    "    # Apply Equal Padding to All Sides\n",
    "    if padding != 0:\n",
    "        imagePadded = np.zeros(\n",
    "            (image.shape[0] + padding*2, image.shape[1] + padding*2))\n",
    "        imagePadded[int(padding):int(-1 * padding),\n",
    "                    int(padding):int(-1 * padding)] = image\n",
    "        # print(imagePadded)\n",
    "    else:\n",
    "        imagePadded = image\n",
    "\n",
    "    # Iterate through image\n",
    "    for y in range(image.shape[1]):\n",
    "        # Exit Convolution\n",
    "        if y > image.shape[1] - yKernShape + 2*padding:\n",
    "            break\n",
    "        # Only Convolve if y has gone down by the specified Strides\n",
    "\n",
    "        for x in range(image.shape[0]):\n",
    "            # Go to next row once kernel is out of bounds\n",
    "            if x > image.shape[0] - xKernShape + 2*padding:\n",
    "                break\n",
    "            try:\n",
    "                # Only Convolve if x has moved by the specified Strides\n",
    "                output[x, y] = (\n",
    "                    kernel * imagePadded[x: x + xKernShape, y: y + yKernShape]\n",
    "                ).sum()\n",
    "            except:\n",
    "                break\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will convolve matrices of floating point number of different precisions using the same convolve2D function, and observe the performance (time and memory utilization) as well as the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A =  np.random.random_sample(size=(100,100))\n",
    "B = np.random.random_sample(size=(5,5))\n",
    "\n",
    "your_result = convolve2D(A, B, padding=2)\n",
    "\n",
    "print(your_result)\n",
    "print(your_result.shape)\n",
    "\n",
    "np.allclose(your_result, convolve2d(A, B, mode=\"same\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(your_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_conv(datatype):\n",
    "    start_time = time.time()\n",
    "    print(\"--- Testing {} ---\".format(datatype))\n",
    "    for _ in range(5):\n",
    "\n",
    "        A =  np.random.random_sample(size=(1000, 500)).astype(datatype)\n",
    "        B = np.random.random_sample(size=(10,10)).astype(datatype)\n",
    "\n",
    "        your_result = convolve2D(A, B, padding=2)\n",
    "\n",
    "    print(\"--- Done: The execution took %s seconds ---\" %\n",
    "          (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we use the function \"memory_usage\" to measure the memory utilization during the execution of the chosen function. At the same time, we record the runtime of each trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_types = [\"float16\", \"float32\", \"float64\", \"float128\"]\n",
    "\n",
    "memory_utilization_tracker = []\n",
    "for float_type in float_types:\n",
    "    mem_usage = memory_usage((test_conv, (float_type,), ), timestamps=False, interval=1)\n",
    "    memory_utilization_tracker.append(mem_usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What do you notice about the run time among the trials? Does the result match your expectations?**\n",
    "\n",
    "*your answer:*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effects of datatypes on 2D convolution results - Memory usage\n",
    "\n",
    "Now let's compare the memory utilization over time using plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "for mem_usage, data_type in zip(memory_utilization_tracker, float_types):\n",
    "    plt.plot(list(range(len(mem_usage))), mem_usage, label=data_type, linewidth=2)\n",
    "    \n",
    "plt.legend()\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What do you notice? Can you explain the difference in the memory usage?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effects of datatypes on 2D convolution results - Precision of result\n",
    "\n",
    "We gained (low) memory usage and (high) speed when using lower precision (16/32/64 vs. 128), what did we lose? Correctness of the solution.\n",
    "\n",
    "Let's see how different the solutions with different datatypes are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_128 =  np.random.random_sample(size=(500,500)).astype(\"float128\")\n",
    "B_128 = np.random.random_sample(size=(10,10)).astype(\"float128\")\n",
    "A_64 =  A_128.astype(\"float64\")\n",
    "B_64 = B_128.astype(\"float64\")\n",
    "A_32 =  A_128.astype(\"float32\")\n",
    "B_32 = B_128.astype(\"float32\")\n",
    "A_16 =  A_128.astype(\"float16\")\n",
    "B_16 = B_128.astype(\"float16\")\n",
    "\n",
    "result_128 = convolve2D(A_128, B_128, padding=2)\n",
    "result_64 = convolve2D(A_64, B_64, padding=2)\n",
    "result_32 = convolve2D(A_32, B_32, padding=2)\n",
    "result_16 = convolve2D(A_16, B_16, padding=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's see the cost of the efficiency gain by using lower-precision data types. We will do this by comparing the result of lower-precision datatypes to that of the highest precision datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((np.abs(result_128 - result_64)).sum())\n",
    "print((np.abs(result_128 - result_32)).sum())\n",
    "print((np.abs(result_128 - result_16)).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What do you notice? Can you explain what you see?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer:*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "# References\n",
    "- https://docs.python.org/3/tutorial/floatingpoint.html\n",
    "- https://diybigdata.net/2016/07/data-science-and-floating-point-arithmetic/\n",
    "- https://www.lahey.com/float.htm\n",
    "- https://numpy.org/doc/stable/user/basics.types.html\n",
    "- https://numpy.org/doc/stable/reference/arrays.scalars.html#integer-types"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "90a948cbea380b723655a2e5bb8f62bc386f4a090480d19b8abbba0f04c678da"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
